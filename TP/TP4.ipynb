{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6BfoslGdGug"
   },
   "source": [
    "# TP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SDsVwmAc_au"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e823qpntc_av"
   },
   "source": [
    "# Entraîner un classificateur\n",
    "\n",
    "Ça y est. Vous avez vu comment définir des réseaux de neurones, calculer les pertes et faire\n",
    "mises à jour des poids du réseau.\n",
    "\n",
    "\n",
    "## Et les données ?\n",
    "\n",
    "Généralement, lorsque vous devez traiter des données d'image, de texte, audio ou vidéo,\n",
    "vous pouvez utiliser des packages Python standard qui chargent des données dans un tableau numpy.\n",
    "Ensuite, vous pouvez convertir ce tableau en ``torch.*Tensor``.\n",
    "\n",
    "- Pour les images, des packages tels que Pillow, OpenCV sont utiles\n",
    "- Pour l'audio, des packages tels que scipy et librosa\n",
    "\n",
    "\n",
    "Spécifiquement pour la vision, un package appelé\n",
    "``torchvision``, possède des chargeurs de données pour les ensembles de données courants tels que\n",
    "ImageNet, CIFAR10, MNIST, etc. et transformateurs de données pour les images, à savoir,\n",
    "``torchvision.datasets`` et ``torch.utils.data.DataLoader``. C'est très pratique !\n",
    "\n",
    "Pour ce tutoriel, nous utiliserons l'ensemble de données CIFAR10.\n",
    "Voir des exemples sur le [lien](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "\n",
    "## Entraîner un classificateur d'images\n",
    "\n",
    "Nous allons suivre les étapes suivantes dans l'ordre :\n",
    "\n",
    "1. Charger et normaliser les ensembles de données de formation et de test CIFAR10 à l'aide\n",
    "   ``torchvision``\n",
    "2. Définir un réseau neuronal convolutif\n",
    "3. Définir une fonction de perte\n",
    "4. Entraîner le réseau sur le training set\n",
    "5. Testez le réseau sur les données de test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EHRVyLstZYZ"
   },
   "source": [
    "### 1. Load and normalize CIFAR10\n",
    "\n",
    "En utilisant ``torchvision``, il est très facile de travailler avec Cifar10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CRT7stTc_aw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFwmjHj2c_aw"
   },
   "source": [
    "La sortie des ensembles de données torchvision sont des images PIL Image à valeurs dans [0, 1].\n",
    "Nous les transformons en tensors normalisés à valeurs dans [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1nczoMcc_ax"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St222F-JSY7p"
   },
   "source": [
    "trainloader et testloader nous permettront d’échantillonner des lots de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bxd8HZxuex1"
   },
   "source": [
    " <font color='blue'> **Question** : Quelles sont les classes du dataset ? ([link](https://www.cs.toronto.edu/~kriz/cifar.html)) </font>\n",
    "\n",
    "  <font color='blue'> **Question** : Combien d'images sont dans le training set ? </font>\n",
    "  \n",
    "  <font color='blue'> **Question** : Combien d'images sont dans le test set  ? </font>\n",
    "\n",
    "  <font color='blue'> **Question** : Que fait ```transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))``` ? Comment cela serait-il codé pour des images en noir et blanc ? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCyq52E-c_ax"
   },
   "source": [
    "Regardons quelques images du training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OViYagGMc_ax"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SK8qI7oUYTbD"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjq1Pm_tc_ax"
   },
   "source": [
    "### 2. Define a Convolutional Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVO7HGN-DsoH"
   },
   "source": [
    " <font color='blue'> **Question** : Compléter la cellule suivante </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sms1esepc_ay"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(__, 120) #TODO\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, __) #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Q6bsAwuadUa"
   },
   "outputs": [],
   "source": [
    "net(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaEdNBUEc_ay"
   },
   "source": [
    "### 3. Define a Loss function and optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYIPeA_fc_ay"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkJNit8IHi17"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsESdHFYc_az"
   },
   "source": [
    "### 4. Entraînement du réseau\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz8yHrEec_az"
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5HaG97Yc_az"
   },
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LPu7V86t_Mi"
   },
   "source": [
    " <font color='blue'> **Question** : Que fait la cellule précedente ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsCjLkUic_az"
   },
   "source": [
    "See [here](https://pytorch.org/docs/stable/notes/serialization.html)\n",
    "for more details on saving PyTorch models.\n",
    "\n",
    "### 5. Test du réseau\n",
    "\n",
    "Nous avons entraîné le réseau avec 2 passages sur l'ensemble de données de formation.\n",
    "Vérifions si le réseau a appris quelque chose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hSRatv3c_a0"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnrGRYgqc_a0"
   },
   "source": [
    "Ensuite, chargeons à nouveau notre modèle enregistré (remarque : sauvegarder et recharger le modèle\n",
    "n'était pas nécessaire ici, nous l'avons fait uniquement pour illustrer comment procéder) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcAvphgHc_a0"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itm8j58zc_a0"
   },
   "source": [
    "D'accord, voyons maintenant ce que le réseau neuronal pense de ces exemples ci-dessus :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrVRrHdOc_a0"
   },
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "y4-r3vOzEHso"
   },
   "source": [
    " <font color='blue'> **Question** : Quelles sont les probabilités que la première image du batch se trouve dans chaque classe ? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lE6vZhR4c_a0"
   },
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQiUexdjc_a1"
   },
   "source": [
    "Observons la matrice de confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtDU-3emNVjn"
   },
   "source": [
    " <font color='blue'> **Question** : Qu'est-ce qui est fait dans la cellule suivante ? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vb2EPeWPc_a1"
   },
   "outputs": [],
   "source": [
    "all_labels = torch.tensor([])\n",
    "all_predicted = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_labels = torch.cat((all_labels, labels))\n",
    "        all_predicted = torch.cat((all_predicted, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0BIABu9N6QZ"
   },
   "source": [
    " <font color='blue'> **Question** : Compléter la cellule ci-dessous. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfIVT8qyNbNs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "t_test = #TODO\n",
    "t_pred = #TODO\n",
    "\n",
    "\n",
    "#Classification report\n",
    "print(classification_report(t_test,t_pred,target_names = classes))\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(t_test,t_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels = classes)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
