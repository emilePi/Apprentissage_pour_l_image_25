{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F87uO5ljPlvt"
   },
   "source": [
    "# TP2\n",
    "\n",
    "Ce TP est inspiré du tutoriel Pytorch disponible [ici](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). Le but est d'apprendre à manier les \"tensors\" et leur backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T115WaqsOhPv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl9a0Kl6OO4o"
   },
   "source": [
    "\n",
    "# Tensors\n",
    "\n",
    "Les tensors sont une structure de données spécialisée très similaire aux tableaux\n",
    "et des matrices. Dans PyTorch, nous utilisons des tenseurs pour coder les entrées et\n",
    "les sorties d’un modèle, ainsi que les paramètres du modèle.\n",
    "\n",
    "Les tenseurs sont similaires aux ndarrays de NumPy, sauf qu'ils peuvent s'exécuter sur\n",
    "GPU ou autre matériel spécialisé pour accélérer le calcul.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iaoU0SXOO4p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yfjWhltOO4p"
   },
   "source": [
    "## Tensor Initialization\n",
    "Les tenseurs peuvent être initialisés de différentes manières. Jetez un œil aux exemples suivants :\n",
    "\n",
    "**Directement à partir des données**\n",
    "\n",
    "Les tenseurs peuvent être créés directement à partir des données. Le type de données est automatiquement déduit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4dcxn2aOO4p"
   },
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BTBkkz4OO4p"
   },
   "source": [
    "\n",
    "**À partir d'un tableau NumPy**\n",
    "\n",
    "Les tenseurs peuvent être créés à partir de tableaux NumPy (et vice versa - voir les sections suivantes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H6sBUeNOO4p"
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6ark1_GOO4q"
   },
   "source": [
    "**Avec des valeurs aléatoires ou constantes :**\n",
    "\n",
    "``shape`` est un tuple de dimensions tensorielles. Dans les fonctions ci-dessous, il détermine la dimensionnalité du tenseur de sortie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxKShDm1OO4q"
   },
   "outputs": [],
   "source": [
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "norm_tensor = torch.norm(rand_tensor)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "print(f\"Norm Tensor: \\n {norm_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7tcHwDvOO4q"
   },
   "source": [
    "**À partir d'un autre tenseur :**\n",
    "\n",
    "Le nouveau tenseur conserve les propriétés (forme, type de données) du tenseur d'argument, à moins qu'il ne soit explicitement remplacé.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYb01GGgOO4q"
   },
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSeQP5vCOO4q"
   },
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf8HjgeoOO4r"
   },
   "source": [
    "## Tensor Operations\n",
    "\n",
    "\n",
    "Plus de 100 opérations tensorielles, dont la transposition, l'indexation, le découpage,\n",
    "les opérations mathématiques, l'algèbre linéaire, l'échantillonnage aléatoire, etc. sont décrits en détail\n",
    "[ici](https://pytorch.org/docs/stable/torch.html)_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axcPk_TqOO4r"
   },
   "source": [
    "Ci-dessous, quelques exemples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTzoiOLjOO4s"
   },
   "source": [
    "**Standard numpy-like indexing and slicing:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rNek26lOO4s"
   },
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvzyg6eJOO4s"
   },
   "source": [
    "**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n",
    "See also [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)_,\n",
    "another tensor joining op that is subtly different from ``torch.cat``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QweI0MSBOO4s"
   },
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo2GKHcaOO4s"
   },
   "source": [
    "**Multiplying tensors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc-AgLptOO4s"
   },
   "outputs": [],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EfQOFknOO4s"
   },
   "source": [
    "This computes the matrix multiplication between two tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPvHRH4-OO4s"
   },
   "outputs": [],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XR-7dy5nOO4t"
   },
   "source": [
    "**In-place operations**\n",
    "Operations that have a ``_`` suffix are in-place. For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hsySY5GOO4t"
   },
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myiVx5GQOO4t"
   },
   "source": [
    "\n",
    "## Bridge with NumPy\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory\n",
    "locations, and changing one will change\tthe other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4U7diODOO4t"
   },
   "source": [
    "### Tensor to NumPy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6Ug12aAOO4t"
   },
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGl39Cl3OO4u"
   },
   "source": [
    "A change in the tensor reflects in the NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QzX6CmSOO4u"
   },
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix9N7bAiOO4u"
   },
   "source": [
    "### NumPy array to Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjqAWZIFOO4u"
   },
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUnrJJpNOO4v"
   },
   "source": [
    "Changes in the NumPy array reflects in the tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEpofh1GOO4v"
   },
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntk_pfuGOYBo"
   },
   "source": [
    "\n",
    "# Une introduction à ``torch.autograd``\n",
    "\n",
    "``torch.autograd`` est le module de différenciation automatique de PyTorch qui permet d'entraîner les réseaux de neurones.\n",
    "\n",
    "## Background\n",
    "\n",
    "Les réseaux de neurones (NN) sont un ensemble de fonctions imbriquées qui sont\n",
    "exécutées sur une donnée d'entrée. Ces fonctions sont définies par des **paramètres** (composés de poids et de biais), qui dans PyTorch sont stockés dans des tenseurs.\n",
    "\n",
    "L'entraînement d'un NN se déroule en deux étapes :\n",
    "\n",
    "**Forward propagation** :  Il exécute les données d'entrée dans chacune de ses\n",
    "fonctions.\n",
    "\n",
    "**Backward propagation** : Pour calculer le gradient de chaque paramètre, le réseau est parcouru en arrière à partir de la sortie, en collectant les dérivées de l'erreur à chaque couche.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOaByaaOYBs"
   },
   "source": [
    "## Differentiation in Autograd\n",
    "\n",
    "Jetons un coup d'œil à la façon dont ``autograd`` collecte les gradients. On crée deux tenseurs ``a`` et ``b`` avec\n",
    "``requires_grad=True``. Cela signale à ``autograd`` que chaque opération sur eux doit être suivie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CbVVbWvOYBs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X26DaP6DOYBs"
   },
   "source": [
    "On créé ensuite le tenseur ``Q`` (à partir de ``a`` et ``b``).\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ6xrGxpOYBt"
   },
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXxsNl2KOYBt"
   },
   "source": [
    "On a alors :\n",
    "\n",
    "\\begin{align}\\nabla_{a}Q = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\nabla_{b}Q = -2b\\end{align}\n",
    "\n",
    "\n",
    "Lorsque nous appelons ``.backward()`` sur ``Q``, autograd calcule ces gradients\n",
    "et les stocke dans l'attribut ``.grad`` des tenseurs respectifs.\n",
    "\n",
    "Nous devons passer explicitement un argument ``gradient`` dans ``Q.backward()`` car c'est un vecteur.\n",
    "``gradient`` est un tenseur de la même forme que ``Q``, et il représente le\n",
    "gradient de Q par rapport à lui-même, c'est-à-dire\n",
    "\n",
    "\\begin{align}\\nabla_Q Q = \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1\\end{pmatrix}\\end{align}\n",
    "\n",
    "\n",
    "Quand Q est un réel, cet argument n'est pas obligatoire alors une astuce est de faire ``Q.sum().backward()``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1B4PMs3OYBt"
   },
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "#or : Q.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNECllDBOYBt"
   },
   "source": [
    "Gradients are now deposited in ``a.grad`` and ``b.grad``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRny86EEOYBt"
   },
   "outputs": [],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6Ro1p-N3S8Z"
   },
   "source": [
    " <font color='blue'> **Question** : Exécuter la cellule ci-dessous. Expliquer l'erreur. Comment l'éviter ? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl0qBpAellWK"
   },
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO7ToDmKFEuc"
   },
   "source": [
    " <font color='blue'> **Question** : Expliquer la cellule suivante. A quoi faut-il faire attention pour calculer plusieurs gradients de suite ?  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6kOL0VoEzje"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "# First backward\n",
    "Q = 3*a**3 - b**2\n",
    "Q.sum().backward()\n",
    "\n",
    "print(a.grad)\n",
    "\n",
    "# Second backward\n",
    "Q = 3*a**3 - b**2\n",
    "Q.sum().backward()\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNj8ZAZz6RF-"
   },
   "source": [
    "## Usage in PyTorch\n",
    "\n",
    "\n",
    "Jetons un coup d'œil à une seule étape d'entraînement.\n",
    "Pour cet exemple, nous chargeons un modèle resnet18 pré-entraîné depuis ``torchvision``.\n",
    "Nous créons un tenseur de données aléatoires pour représenter une seule image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBHvgMiGOYBp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT80S6jW60fn"
   },
   "source": [
    " <font color='blue'> **Question** : Expliquer la cellule précédente. Resnet18 est un réseau de classification. Quelles sont les tailles des images sur lesquelles il travaille ? Combien y a-t-il de couches ? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQxAgc66mbwi"
   },
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9QBabATOYBp"
   },
   "source": [
    "Ensuite, nous exécutons les données d'entrée dans le modèle à travers chacune de ses couches pour faire une prédiction.\n",
    "C'est le **forward pass**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-YRMPD9OYBq"
   },
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass\n",
    "print(torch.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp5YG-1N5F8G"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from torchvision.transforms import Resize\n",
    "from IPython.display import display\n",
    "change = Resize((64*6,64*6))\n",
    "data_image = to_pil_image(change(data[0,:,:,:]))\n",
    "display(data_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_0lW1Ye73f6"
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.plot(prediction[0].detach().numpy(),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntE71t7q7cj7"
   },
   "source": [
    "<font color='blue'> **Question** : A quelle classe appartiennent la donnée selon le modèle ? ([Un lien](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFYj6DBrmfbb"
   },
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS_H44ifOYBq"
   },
   "source": [
    "\n",
    "\n",
    "Nous utilisons la prédiction du modèle et l'étiquette correspondante pour calculer l'erreur (``loss``).\n",
    "L'étape suivante consiste à rétropropager cette erreur à travers le réseau.\n",
    "La backpropagation est déclenchée lorsque nous appelons ``.backward()`` sur le tenseur d'erreur.\n",
    "Autograd calcule et stocke ensuite les gradients pour chaque paramètre du modèle dans l'attribut ``.grad`` du paramètre.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yjz8IJHNOYBq"
   },
   "outputs": [],
   "source": [
    "loss = ((prediction - labels)**2).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE-MEpuIOYBq"
   },
   "source": [
    "Ensuite, nous chargeons un optimiseur avec un taux d'apprentissage de 0,01. Nous enregistrons tous les paramètres du modèle dans l'optimiseur.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x7EGKZ1OYBr"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY3C1a6v8MMC"
   },
   "source": [
    "<font color='blue'> **Question** : Que signifie SGD ? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47Ap_2C9OYBr"
   },
   "source": [
    "\n",
    "Enfin, nous appelons ``.step()`` pour lancer la descente de gradient. L'optimiseur ajuste chaque paramètre par son dégradé stocké dans ``.grad``.\n",
    "\n",
    "Tout cela est automatique !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-oiWB3KOYBs"
   },
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pyFwu5HnGsu"
   },
   "source": [
    "Voici donc comment s'écrira un entraînement de réseau :    \n",
    "\n",
    "Jusqu'à convergence :    \n",
    "- Pour chaque batch $\\mathcal{S} \\subset \\mathcal{T}$\n",
    "    - prediction = model($\\mathcal{S}$)\n",
    "    - Calculer Loss($\\mathcal{S}$, prediction)\n",
    "    - optim.step()\n",
    "    - <font color='blue'> un truc </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua2e-VXcOYBs"
   },
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iz_PJaX5nThH"
   },
   "source": [
    "# <font color='blue'> Exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJUuvwRbtdW7"
   },
   "source": [
    "Résolvons $Ax = b$\n",
    "avec\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "   2 & -1 & 0 & 0 & \\cdots & 0 & 0\\\\\n",
    "   -1 & 2 & -1 & 0 &\\cdots & 0 & 0 \\\\\n",
    "   0 & -1 & 2 & -1 &\\cdots & 0 & 0 \\\\\n",
    "   0 & 0 & -1 & 2 &\\cdots & 0 & 0 \\\\\n",
    "   \\vdots  & \\vdots & \\vdots & \\vdots& \\ddots & \\vdots & \\vdots  \\\\\n",
    "   0 & 0 & 0 & 0 & \\cdots & 2 & -1 \\\\\n",
    "   0 & 0 & 0 & 0 & \\cdots & -1 & 2\n",
    " \\end{pmatrix} \\in \\mathbb{R}^{n \\times n} = \\mathscr{M}_n(\\mathbb{R})\n",
    " \\text{ and }\n",
    " b =\n",
    " \\begin{pmatrix}\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    " \\vdots \\\\\n",
    " 1 \\\\\n",
    " 1\n",
    " \\end{pmatrix} \\in \\mathbb{R}^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZSqeyhlwgnB"
   },
   "source": [
    "Il est suffisant de minimiser la fonction :       \n",
    "$$F : x \\in \\mathbb{R}^n \\mapsto \\frac{1}{2}\\langle Ax,x \\rangle - \\langle b, x \\rangle$$\n",
    "\n",
    "1. Implémentez la matrice ```A``` et le vecteur ```b``` dans ```torch``` avec $n = 20$\n",
    "2. Calculez le gradient d'un vecteur aléatoire ```x``` avec ```torch.autograd```\n",
    "3. Vérifiez que le calcul du gradient est correct.\n",
    "4. Implémenter une descente de gradient à pas constant pour résoudre le problème avec ```10**3``` itérations et un taux d'apprentissage égal à ```0.1``` .\n",
    "5. Implémentez-le en utilisant ```torch.optim.SGD```, un taux d'apprentissage égal à 0,1, un momentum égal à 0,9 et ```10**3``` itérations.\n",
    "6. Tracez la solution $x$. Reconnaissez-vous $A$ ? Expliquez la forme de la courbe.\n",
    "\n",
    "**Questions supplémentaires :**\n",
    "7. Pourquoi suffit-il de minimiser $F$ pour résoudre le problème ?\n",
    "8. Justifiez que $A$ est définitie positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xOQzbh-soBw"
   },
   "source": [
    "# Retour sur le TP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs, make_gaussian_quantiles\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "seed = 32\n",
    "torch.manual_seed(seed)   \n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = make_gaussian_quantiles(n_features=2, n_classes=3, n_samples=500)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=.4, random_state=12)\n",
    "# Number of points in each set:\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "t_train = torch.tensor(t_train, dtype=torch.int64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "t_test = torch.tensor(t_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d = 6\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,d)\n",
    "        self.fc2 = nn.Linear(d, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net2 = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='blue'> Question : Reconnaissez-vous le réseau ci-dessus ?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc1.bias.requires_grad = False\n",
    "net.fc1.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> Question : Qu'est-ce qui est fait ci-dessus ?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "W1 =torch.randn(d,2)\n",
    "b1 = torch.randn(d)\n",
    "net.fc2.bias = torch.nn.Parameter(b1)\n",
    "net.fc2.weight = torch.nn.Parameter(W1)\n",
    "net2.fc2.bias = torch.nn.Parameter(b1)\n",
    "net2.fc2.weight = torch.nn.Parameter(W1)\n",
    "\n",
    "\n",
    "W2 =torch.randn(3,d)\n",
    "b2 = torch.randn(3)\n",
    "net.fc2.bias = torch.nn.Parameter(b2)\n",
    "net.fc2.weight = torch.nn.Parameter(W2)\n",
    "net2.fc2.bias = torch.nn.Parameter(b2)\n",
    "net2.fc2.weight = torch.nn.Parameter(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.75)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(10**3):  # loop over the dataset multiple times\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    output = net(X_train)\n",
    "    loss = criterion(output, t_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # forward + backward + optimize\n",
    "    output2 = net2(X_train)\n",
    "    loss2 = criterion(output2, t_train)\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize results:\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = 0.02\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "X_grid = np.hstack((xx.ravel(), yy.ravel()))\n",
    "\n",
    "N_grid = xx.ravel().shape[0]\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "feature_transform = lambda x : (net(torch.tensor(x, dtype=torch.float32).unsqueeze(0)).detach().numpy())\n",
    "feature_transform2 = lambda x : (net2(torch.tensor(x, dtype=torch.float32).unsqueeze(0)).detach().numpy())\n",
    "\n",
    "\n",
    "Phi_grid = feature_transform(X_grid)\n",
    "Phi_grid2 = feature_transform2(X_grid)\n",
    "\n",
    "Z =np.argmax(Phi_grid,axis=2)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "figure = plt.figure(figsize=(16, 8))\n",
    "plt.title('Not trained, d = '+str(d))\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.set_title(\"Input data\")\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
    "\n",
    "plt.axis('scaled')\n",
    "ax = plt.subplot(1,2,2)\n",
    "cmap = ListedColormap(['b','y','r','m','g','c'])\n",
    "plt.contourf(xx,yy,Z,  cmap = cmap, alpha=.8)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
    "plt.axis('scaled')\n",
    "\n",
    "Z2 =np.argmax(Phi_grid2,axis=2)\n",
    "Z2 = Z2.reshape(xx.shape)\n",
    "figure = plt.figure(figsize=(16, 8))\n",
    "plt.title('Trained, d = '+str(d))\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.set_title(\"Input data\")\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
    "plt.axis('scaled')\n",
    "ax = plt.subplot(1,2,2)\n",
    "cmap = ListedColormap(['b','y','r','m','g','c'])\n",
    "plt.contourf(xx,yy,Z2,  cmap = cmap, alpha=.8)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
    "plt.axis('scaled')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> Question : Combien de paramètres réels contiennent chaque réseau ?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0OxvDORK3zzp"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
